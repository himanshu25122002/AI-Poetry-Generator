{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34cf500-6cf3-472e-9309-8f612fa23621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3344907e-db27-4442-89d3-07b45b683627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\himan\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\himan\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\himan\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\himan\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7bde990-b102-432f-9f9d-157fd3f54516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0921404-117f-4679-a378-641eefcdcd44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from tensorflow) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.73.0-cp310-cp310-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp310-cp310-win_amd64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp310-cp310-win_amd64.whl.metadata (22 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.16.0-cp310-cp310-win_amd64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp310-cp310-win_amd64.whl (375.7 MB)\n",
      "   ---------------------------------------- 0.0/375.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/375.7 MB 16.7 MB/s eta 0:00:23\n",
      "   ---------------------------------------- 1.0/375.7 MB 16.7 MB/s eta 0:00:23\n",
      "   ---------------------------------------- 1.0/375.7 MB 16.7 MB/s eta 0:00:23\n",
      "   ---------------------------------------- 2.9/375.7 MB 3.4 MB/s eta 0:01:52\n",
      "    --------------------------------------- 5.0/375.7 MB 5.0 MB/s eta 0:01:15\n",
      "    --------------------------------------- 5.8/375.7 MB 4.6 MB/s eta 0:01:20\n",
      "    --------------------------------------- 5.8/375.7 MB 4.6 MB/s eta 0:01:20\n",
      "    --------------------------------------- 5.8/375.7 MB 4.6 MB/s eta 0:01:20\n",
      "    --------------------------------------- 6.0/375.7 MB 3.3 MB/s eta 0:01:52\n",
      "    --------------------------------------- 6.6/375.7 MB 3.2 MB/s eta 0:01:56\n",
      "    --------------------------------------- 8.1/375.7 MB 3.5 MB/s eta 0:01:44\n",
      "    --------------------------------------- 9.2/375.7 MB 3.9 MB/s eta 0:01:36\n",
      "    --------------------------------------- 9.2/375.7 MB 3.9 MB/s eta 0:01:36\n",
      "   - -------------------------------------- 10.5/375.7 MB 3.6 MB/s eta 0:01:42\n",
      "   - -------------------------------------- 13.1/375.7 MB 4.2 MB/s eta 0:01:27\n",
      "   - -------------------------------------- 14.2/375.7 MB 4.3 MB/s eta 0:01:25\n",
      "   - -------------------------------------- 14.9/375.7 MB 4.2 MB/s eta 0:01:26\n",
      "   - -------------------------------------- 15.5/375.7 MB 4.2 MB/s eta 0:01:26\n",
      "   - -------------------------------------- 16.0/375.7 MB 4.1 MB/s eta 0:01:28\n",
      "   - -------------------------------------- 16.8/375.7 MB 4.0 MB/s eta 0:01:29\n",
      "   - -------------------------------------- 17.6/375.7 MB 4.1 MB/s eta 0:01:28\n",
      "   - -------------------------------------- 18.6/375.7 MB 4.1 MB/s eta 0:01:28\n",
      "   -- ------------------------------------- 19.9/375.7 MB 4.2 MB/s eta 0:01:25\n",
      "   -- ------------------------------------- 21.5/375.7 MB 4.3 MB/s eta 0:01:23\n",
      "   -- ------------------------------------- 22.3/375.7 MB 4.3 MB/s eta 0:01:23\n",
      "   -- ------------------------------------- 23.1/375.7 MB 4.3 MB/s eta 0:01:22\n",
      "   -- ------------------------------------- 24.4/375.7 MB 4.3 MB/s eta 0:01:21\n",
      "   -- ------------------------------------- 25.7/375.7 MB 4.4 MB/s eta 0:01:20\n",
      "   -- ------------------------------------- 26.5/375.7 MB 4.5 MB/s eta 0:01:19\n",
      "   -- ------------------------------------- 27.3/375.7 MB 4.4 MB/s eta 0:01:20\n",
      "   -- ------------------------------------- 28.0/375.7 MB 4.4 MB/s eta 0:01:20\n",
      "   --- ------------------------------------ 29.1/375.7 MB 4.4 MB/s eta 0:01:19\n",
      "   --- ------------------------------------ 30.1/375.7 MB 4.4 MB/s eta 0:01:19\n",
      "   --- ------------------------------------ 30.9/375.7 MB 4.4 MB/s eta 0:01:19\n",
      "   --- ------------------------------------ 32.0/375.7 MB 4.4 MB/s eta 0:01:18\n",
      "   --- ------------------------------------ 32.8/375.7 MB 4.4 MB/s eta 0:01:18\n",
      "   --- ------------------------------------ 33.3/375.7 MB 4.4 MB/s eta 0:01:19\n",
      "   --- ------------------------------------ 34.6/375.7 MB 4.4 MB/s eta 0:01:18\n",
      "   --- ------------------------------------ 36.2/375.7 MB 4.5 MB/s eta 0:01:16\n",
      "   --- ------------------------------------ 37.5/375.7 MB 4.6 MB/s eta 0:01:15\n",
      "   ---- ----------------------------------- 38.8/375.7 MB 4.6 MB/s eta 0:01:14\n",
      "   ---- ----------------------------------- 39.6/375.7 MB 4.6 MB/s eta 0:01:14\n",
      "   ---- ----------------------------------- 39.8/375.7 MB 4.5 MB/s eta 0:01:15\n",
      "   ---- ----------------------------------- 40.4/375.7 MB 4.5 MB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 40.9/375.7 MB 4.4 MB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 41.7/375.7 MB 4.4 MB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 42.5/375.7 MB 4.4 MB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 43.8/375.7 MB 4.4 MB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 45.1/375.7 MB 4.5 MB/s eta 0:01:14\n",
      "   ---- ----------------------------------- 46.7/375.7 MB 4.5 MB/s eta 0:01:13\n",
      "   ----- ---------------------------------- 48.5/375.7 MB 4.6 MB/s eta 0:01:12\n",
      "   ----- ---------------------------------- 50.1/375.7 MB 4.7 MB/s eta 0:01:10\n",
      "   ----- ---------------------------------- 51.4/375.7 MB 4.7 MB/s eta 0:01:09\n",
      "   ----- ---------------------------------- 51.9/375.7 MB 4.7 MB/s eta 0:01:10\n",
      "   ----- ---------------------------------- 52.2/375.7 MB 4.6 MB/s eta 0:01:10\n",
      "   ----- ---------------------------------- 52.4/375.7 MB 4.6 MB/s eta 0:01:11\n",
      "   ----- ---------------------------------- 53.0/375.7 MB 4.5 MB/s eta 0:01:12\n",
      "   ----- ---------------------------------- 53.7/375.7 MB 4.5 MB/s eta 0:01:12\n",
      "   ----- ---------------------------------- 54.5/375.7 MB 4.5 MB/s eta 0:01:12\n",
      "   ----- ---------------------------------- 55.8/375.7 MB 4.5 MB/s eta 0:01:11\n",
      "   ------ --------------------------------- 57.1/375.7 MB 4.5 MB/s eta 0:01:11\n",
      "   ------ --------------------------------- 58.7/375.7 MB 4.6 MB/s eta 0:01:09\n",
      "   ------ --------------------------------- 60.3/375.7 MB 4.6 MB/s eta 0:01:08\n",
      "   ------ --------------------------------- 61.1/375.7 MB 4.6 MB/s eta 0:01:08\n",
      "   ------ --------------------------------- 61.6/375.7 MB 4.6 MB/s eta 0:01:09\n",
      "   ------ --------------------------------- 62.4/375.7 MB 4.6 MB/s eta 0:01:09\n",
      "   ------ --------------------------------- 63.4/375.7 MB 4.6 MB/s eta 0:01:09\n",
      "   ------ --------------------------------- 64.5/375.7 MB 4.6 MB/s eta 0:01:08\n",
      "   ------- -------------------------------- 65.8/375.7 MB 4.6 MB/s eta 0:01:07\n",
      "   ------- -------------------------------- 67.4/375.7 MB 4.7 MB/s eta 0:01:07\n",
      "   ------- -------------------------------- 68.7/375.7 MB 4.7 MB/s eta 0:01:06\n",
      "   ------- -------------------------------- 69.7/375.7 MB 4.7 MB/s eta 0:01:06\n",
      "   ------- -------------------------------- 70.5/375.7 MB 4.7 MB/s eta 0:01:06\n",
      "   ------- -------------------------------- 71.0/375.7 MB 4.7 MB/s eta 0:01:06\n",
      "   ------- -------------------------------- 71.8/375.7 MB 4.6 MB/s eta 0:01:06\n",
      "   ------- -------------------------------- 72.4/375.7 MB 4.6 MB/s eta 0:01:06\n",
      "   ------- -------------------------------- 73.4/375.7 MB 4.6 MB/s eta 0:01:06\n",
      "   ------- -------------------------------- 74.7/375.7 MB 4.6 MB/s eta 0:01:05\n",
      "   -------- ------------------------------- 76.0/375.7 MB 4.7 MB/s eta 0:01:05\n",
      "   -------- ------------------------------- 76.8/375.7 MB 4.7 MB/s eta 0:01:05\n",
      "   -------- ------------------------------- 77.3/375.7 MB 4.6 MB/s eta 0:01:05\n",
      "   -------- ------------------------------- 78.1/375.7 MB 4.6 MB/s eta 0:01:05\n",
      "   -------- ------------------------------- 79.2/375.7 MB 4.6 MB/s eta 0:01:05\n",
      "   -------- ------------------------------- 80.2/375.7 MB 4.6 MB/s eta 0:01:04\n",
      "   -------- ------------------------------- 81.5/375.7 MB 4.7 MB/s eta 0:01:04\n",
      "   -------- ------------------------------- 82.8/375.7 MB 4.7 MB/s eta 0:01:03\n",
      "   -------- ------------------------------- 83.9/375.7 MB 4.7 MB/s eta 0:01:03\n",
      "   --------- ------------------------------ 84.9/375.7 MB 4.7 MB/s eta 0:01:03\n",
      "   --------- ------------------------------ 86.0/375.7 MB 4.7 MB/s eta 0:01:02\n",
      "   --------- ------------------------------ 86.8/375.7 MB 4.7 MB/s eta 0:01:02\n",
      "   --------- ------------------------------ 87.6/375.7 MB 4.7 MB/s eta 0:01:02\n",
      "   --------- ------------------------------ 88.6/375.7 MB 4.7 MB/s eta 0:01:02\n",
      "   --------- ------------------------------ 89.7/375.7 MB 4.7 MB/s eta 0:01:02\n",
      "   --------- ------------------------------ 91.2/375.7 MB 4.7 MB/s eta 0:01:01\n",
      "   --------- ------------------------------ 92.3/375.7 MB 4.7 MB/s eta 0:01:01\n",
      "   --------- ------------------------------ 93.1/375.7 MB 4.7 MB/s eta 0:01:01\n",
      "   ---------- ----------------------------- 94.1/375.7 MB 4.7 MB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 95.4/375.7 MB 4.7 MB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 95.9/375.7 MB 4.7 MB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 96.5/375.7 MB 4.7 MB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 97.3/375.7 MB 4.7 MB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 98.3/375.7 MB 4.7 MB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 99.4/375.7 MB 4.7 MB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 100.7/375.7 MB 4.7 MB/s eta 0:00:59\n",
      "   ---------- ----------------------------- 102.0/375.7 MB 4.7 MB/s eta 0:00:59\n",
      "   ---------- ----------------------------- 102.8/375.7 MB 4.7 MB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 103.5/375.7 MB 4.7 MB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 104.3/375.7 MB 4.7 MB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 105.4/375.7 MB 4.7 MB/s eta 0:00:58\n",
      "   ----------- ---------------------------- 106.7/375.7 MB 4.7 MB/s eta 0:00:58\n",
      "   ----------- ---------------------------- 107.7/375.7 MB 4.7 MB/s eta 0:00:58\n",
      "   ----------- ---------------------------- 108.8/375.7 MB 4.7 MB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 109.6/375.7 MB 4.7 MB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 110.6/375.7 MB 4.7 MB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 111.9/375.7 MB 4.7 MB/s eta 0:00:57\n",
      "   ------------ --------------------------- 113.0/375.7 MB 4.7 MB/s eta 0:00:56\n",
      "   ------------ --------------------------- 113.8/375.7 MB 4.7 MB/s eta 0:00:56\n",
      "   ------------ --------------------------- 114.8/375.7 MB 4.7 MB/s eta 0:00:56\n",
      "   ------------ --------------------------- 116.1/375.7 MB 4.7 MB/s eta 0:00:56\n",
      "   ------------ --------------------------- 117.2/375.7 MB 4.7 MB/s eta 0:00:55\n",
      "   ------------ --------------------------- 118.0/375.7 MB 4.7 MB/s eta 0:00:55\n",
      "   ------------ --------------------------- 118.5/375.7 MB 4.7 MB/s eta 0:00:55\n",
      "   ------------ --------------------------- 119.5/375.7 MB 4.7 MB/s eta 0:00:55\n",
      "   ------------ --------------------------- 120.6/375.7 MB 4.7 MB/s eta 0:00:55\n",
      "   ------------ --------------------------- 121.6/375.7 MB 4.7 MB/s eta 0:00:55\n",
      "   ------------- -------------------------- 122.7/375.7 MB 4.7 MB/s eta 0:00:54\n",
      "   ------------- -------------------------- 123.5/375.7 MB 4.7 MB/s eta 0:00:54\n",
      "   ------------- -------------------------- 124.3/375.7 MB 4.7 MB/s eta 0:00:54\n",
      "   ------------- -------------------------- 125.6/375.7 MB 4.7 MB/s eta 0:00:54\n",
      "   ------------- -------------------------- 126.9/375.7 MB 4.7 MB/s eta 0:00:53\n",
      "   ------------- -------------------------- 128.2/375.7 MB 4.7 MB/s eta 0:00:53\n",
      "   ------------- -------------------------- 129.0/375.7 MB 4.7 MB/s eta 0:00:53\n",
      "   ------------- -------------------------- 130.0/375.7 MB 4.7 MB/s eta 0:00:52\n",
      "   ------------- -------------------------- 131.1/375.7 MB 4.7 MB/s eta 0:00:52\n",
      "   -------------- ------------------------- 132.1/375.7 MB 4.7 MB/s eta 0:00:52\n",
      "   -------------- ------------------------- 132.9/375.7 MB 4.7 MB/s eta 0:00:52\n",
      "   -------------- ------------------------- 133.7/375.7 MB 4.7 MB/s eta 0:00:52\n",
      "   -------------- ------------------------- 134.5/375.7 MB 4.7 MB/s eta 0:00:52\n",
      "   -------------- ------------------------- 135.8/375.7 MB 4.7 MB/s eta 0:00:51\n",
      "   -------------- ------------------------- 137.1/375.7 MB 4.7 MB/s eta 0:00:51\n",
      "   -------------- ------------------------- 138.1/375.7 MB 4.7 MB/s eta 0:00:51\n",
      "   -------------- ------------------------- 139.5/375.7 MB 4.7 MB/s eta 0:00:50\n",
      "   -------------- ------------------------- 140.8/375.7 MB 4.8 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 141.6/375.7 MB 4.7 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 142.1/375.7 MB 4.7 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 142.3/375.7 MB 4.8 MB/s eta 0:00:49\n",
      "   --------------- ------------------------ 143.1/375.7 MB 4.8 MB/s eta 0:00:49\n",
      "   --------------- ------------------------ 143.9/375.7 MB 4.8 MB/s eta 0:00:49\n",
      "   --------------- ------------------------ 144.7/375.7 MB 4.7 MB/s eta 0:00:49\n",
      "   --------------- ------------------------ 145.8/375.7 MB 4.7 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 147.1/375.7 MB 4.7 MB/s eta 0:00:49\n",
      "   --------------- ------------------------ 148.6/375.7 MB 4.8 MB/s eta 0:00:48\n",
      "   --------------- ------------------------ 150.2/375.7 MB 4.8 MB/s eta 0:00:47\n",
      "   ---------------- ----------------------- 151.8/375.7 MB 4.9 MB/s eta 0:00:47\n",
      "   ---------------- ----------------------- 152.6/375.7 MB 4.9 MB/s eta 0:00:46\n",
      "   ---------------- ----------------------- 153.4/375.7 MB 4.8 MB/s eta 0:00:47\n",
      "   ---------------- ----------------------- 153.9/375.7 MB 4.9 MB/s eta 0:00:46\n",
      "   ---------------- ----------------------- 154.9/375.7 MB 4.9 MB/s eta 0:00:46\n",
      "   ---------------- ----------------------- 156.0/375.7 MB 4.8 MB/s eta 0:00:46\n",
      "   ---------------- ----------------------- 157.0/375.7 MB 4.8 MB/s eta 0:00:46\n",
      "   ---------------- ----------------------- 158.6/375.7 MB 4.8 MB/s eta 0:00:46\n",
      "   ---------------- ----------------------- 159.6/375.7 MB 4.8 MB/s eta 0:00:45\n",
      "   ----------------- ---------------------- 160.2/375.7 MB 4.8 MB/s eta 0:00:45\n",
      "   ----------------- ---------------------- 160.7/375.7 MB 4.8 MB/s eta 0:00:45\n",
      "   ----------------- ---------------------- 161.5/375.7 MB 4.8 MB/s eta 0:00:45\n",
      "   ----------------- ---------------------- 162.3/375.7 MB 4.8 MB/s eta 0:00:45\n",
      "   ----------------- ---------------------- 163.3/375.7 MB 4.8 MB/s eta 0:00:45\n",
      "   ----------------- ---------------------- 164.4/375.7 MB 4.8 MB/s eta 0:00:45\n",
      "   ----------------- ---------------------- 165.7/375.7 MB 4.8 MB/s eta 0:00:44\n",
      "   ----------------- ---------------------- 167.2/375.7 MB 4.8 MB/s eta 0:00:44\n",
      "   ----------------- ---------------------- 168.6/375.7 MB 4.8 MB/s eta 0:00:43\n",
      "   ------------------ --------------------- 169.3/375.7 MB 4.8 MB/s eta 0:00:43\n",
      "   ------------------ --------------------- 169.9/375.7 MB 4.8 MB/s eta 0:00:43\n",
      "   ------------------ --------------------- 170.4/375.7 MB 4.8 MB/s eta 0:00:43\n",
      "   ------------------ --------------------- 170.4/375.7 MB 4.8 MB/s eta 0:00:43\n",
      "   ------------------ --------------------- 170.9/375.7 MB 4.8 MB/s eta 0:00:44\n",
      "   ------------------ --------------------- 171.4/375.7 MB 4.7 MB/s eta 0:00:44\n",
      "   ------------------ --------------------- 172.2/375.7 MB 4.7 MB/s eta 0:00:44\n",
      "   ------------------ --------------------- 173.0/375.7 MB 4.7 MB/s eta 0:00:43\n",
      "   ------------------ --------------------- 174.3/375.7 MB 4.7 MB/s eta 0:00:43\n",
      "   ------------------ --------------------- 175.1/375.7 MB 4.7 MB/s eta 0:00:43\n",
      "   ------------------ --------------------- 176.4/375.7 MB 4.8 MB/s eta 0:00:42\n",
      "   ------------------ --------------------- 178.0/375.7 MB 4.8 MB/s eta 0:00:42\n",
      "   ------------------- -------------------- 179.8/375.7 MB 4.8 MB/s eta 0:00:42\n",
      "   ------------------- -------------------- 180.9/375.7 MB 4.8 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 181.7/375.7 MB 4.7 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 182.2/375.7 MB 4.8 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 183.0/375.7 MB 4.8 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 184.0/375.7 MB 4.8 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 185.3/375.7 MB 4.8 MB/s eta 0:00:40\n",
      "   ------------------- -------------------- 186.6/375.7 MB 4.8 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 188.0/375.7 MB 4.8 MB/s eta 0:00:39\n",
      "   -------------------- ------------------- 188.5/375.7 MB 4.8 MB/s eta 0:00:39\n",
      "   -------------------- ------------------- 188.7/375.7 MB 4.8 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 189.3/375.7 MB 4.7 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 189.5/375.7 MB 4.7 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 189.5/375.7 MB 4.7 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 190.1/375.7 MB 4.6 MB/s eta 0:00:41\n",
      "   -------------------- ------------------- 190.6/375.7 MB 4.6 MB/s eta 0:00:41\n",
      "   -------------------- ------------------- 191.4/375.7 MB 4.6 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 192.2/375.7 MB 4.7 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 193.5/375.7 MB 4.7 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 194.5/375.7 MB 4.7 MB/s eta 0:00:39\n",
      "   -------------------- ------------------- 196.1/375.7 MB 4.7 MB/s eta 0:00:39\n",
      "   --------------------- ------------------ 197.9/375.7 MB 4.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 199.5/375.7 MB 4.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 200.0/375.7 MB 4.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 200.5/375.7 MB 4.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 201.1/375.7 MB 4.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 201.9/375.7 MB 4.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 202.9/375.7 MB 4.7 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 203.9/375.7 MB 4.7 MB/s eta 0:00:37\n",
      "   --------------------- ------------------ 205.0/375.7 MB 4.7 MB/s eta 0:00:37\n",
      "   --------------------- ------------------ 206.6/375.7 MB 4.7 MB/s eta 0:00:37\n",
      "   ---------------------- ----------------- 208.1/375.7 MB 4.7 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 208.9/375.7 MB 4.7 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 209.5/375.7 MB 4.6 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 210.2/375.7 MB 4.6 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 211.0/375.7 MB 4.7 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 212.1/375.7 MB 4.7 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 213.4/375.7 MB 4.7 MB/s eta 0:00:35\n",
      "   ---------------------- ----------------- 214.4/375.7 MB 4.7 MB/s eta 0:00:35\n",
      "   ---------------------- ----------------- 215.5/375.7 MB 4.7 MB/s eta 0:00:35\n",
      "   ---------------------- ----------------- 216.0/375.7 MB 4.7 MB/s eta 0:00:35\n",
      "   ----------------------- ---------------- 216.8/375.7 MB 4.7 MB/s eta 0:00:35\n",
      "   ----------------------- ---------------- 217.8/375.7 MB 4.7 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 218.6/375.7 MB 4.7 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 220.2/375.7 MB 4.7 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 221.5/375.7 MB 4.7 MB/s eta 0:00:33\n",
      "   ----------------------- ---------------- 222.8/375.7 MB 4.7 MB/s eta 0:00:33\n",
      "   ----------------------- ---------------- 223.6/375.7 MB 4.7 MB/s eta 0:00:33\n",
      "   ----------------------- ---------------- 224.4/375.7 MB 4.7 MB/s eta 0:00:33\n",
      "   ----------------------- ---------------- 224.9/375.7 MB 4.6 MB/s eta 0:00:33\n",
      "   ------------------------ --------------- 225.4/375.7 MB 4.6 MB/s eta 0:00:33\n",
      "   ------------------------ --------------- 226.2/375.7 MB 4.6 MB/s eta 0:00:33\n",
      "   ------------------------ --------------- 227.0/375.7 MB 4.6 MB/s eta 0:00:33\n",
      "   ------------------------ --------------- 228.1/375.7 MB 4.6 MB/s eta 0:00:32\n",
      "   ------------------------ --------------- 229.1/375.7 MB 4.6 MB/s eta 0:00:32\n",
      "   ------------------------ --------------- 230.7/375.7 MB 4.6 MB/s eta 0:00:32\n",
      "   ------------------------ --------------- 232.3/375.7 MB 4.7 MB/s eta 0:00:31\n",
      "   ------------------------ --------------- 233.3/375.7 MB 4.7 MB/s eta 0:00:31\n",
      "   ------------------------ --------------- 234.1/375.7 MB 4.7 MB/s eta 0:00:31\n",
      "   ------------------------- -------------- 234.9/375.7 MB 4.6 MB/s eta 0:00:31\n",
      "   ------------------------- -------------- 235.7/375.7 MB 4.6 MB/s eta 0:00:31\n",
      "   ------------------------- -------------- 236.7/375.7 MB 4.7 MB/s eta 0:00:30\n",
      "   ------------------------- -------------- 237.5/375.7 MB 4.7 MB/s eta 0:00:30\n",
      "   ------------------------- -------------- 238.6/375.7 MB 4.7 MB/s eta 0:00:30\n",
      "   ------------------------- -------------- 240.1/375.7 MB 4.7 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 240.6/375.7 MB 4.7 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 241.4/375.7 MB 4.6 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 242.2/375.7 MB 4.6 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 243.3/375.7 MB 4.7 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 244.8/375.7 MB 4.7 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 246.2/375.7 MB 4.7 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 246.9/375.7 MB 4.7 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 247.5/375.7 MB 4.7 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 248.3/375.7 MB 4.6 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 249.0/375.7 MB 4.6 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 250.3/375.7 MB 4.7 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 251.7/375.7 MB 4.7 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 253.0/375.7 MB 4.7 MB/s eta 0:00:27\n",
      "   --------------------------- ------------ 254.0/375.7 MB 4.7 MB/s eta 0:00:27\n",
      "   --------------------------- ------------ 254.5/375.7 MB 4.7 MB/s eta 0:00:27\n",
      "   --------------------------- ------------ 254.8/375.7 MB 4.6 MB/s eta 0:00:27\n",
      "   --------------------------- ------------ 255.3/375.7 MB 4.6 MB/s eta 0:00:27\n",
      "   --------------------------- ------------ 255.9/375.7 MB 4.6 MB/s eta 0:00:27\n",
      "   --------------------------- ------------ 256.4/375.7 MB 4.6 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 257.4/375.7 MB 4.6 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 258.5/375.7 MB 4.6 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 259.8/375.7 MB 4.6 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 261.1/375.7 MB 4.6 MB/s eta 0:00:25\n",
      "   --------------------------- ------------ 262.9/375.7 MB 4.7 MB/s eta 0:00:25\n",
      "   ---------------------------- ----------- 264.0/375.7 MB 4.7 MB/s eta 0:00:25\n",
      "   ---------------------------- ----------- 264.5/375.7 MB 4.6 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 265.3/375.7 MB 4.6 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 266.1/375.7 MB 4.6 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 266.9/375.7 MB 4.6 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 267.9/375.7 MB 4.6 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 269.2/375.7 MB 4.6 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 270.5/375.7 MB 4.6 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 271.1/375.7 MB 4.6 MB/s eta 0:00:23\n",
      "   ---------------------------- ----------- 272.1/375.7 MB 4.6 MB/s eta 0:00:23\n",
      "   ----------------------------- ---------- 272.9/375.7 MB 4.6 MB/s eta 0:00:23\n",
      "   ----------------------------- ---------- 273.9/375.7 MB 4.6 MB/s eta 0:00:23\n",
      "   ----------------------------- ---------- 275.3/375.7 MB 4.6 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 276.6/375.7 MB 4.6 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 277.6/375.7 MB 4.6 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 278.7/375.7 MB 4.6 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 279.7/375.7 MB 4.6 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 281.0/375.7 MB 4.6 MB/s eta 0:00:21\n",
      "   ------------------------------ --------- 281.8/375.7 MB 4.7 MB/s eta 0:00:21\n",
      "   ------------------------------ --------- 282.3/375.7 MB 4.7 MB/s eta 0:00:21\n",
      "   ------------------------------ --------- 283.1/375.7 MB 4.7 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 283.6/375.7 MB 4.6 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 284.4/375.7 MB 4.6 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 285.2/375.7 MB 4.6 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 286.3/375.7 MB 4.6 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 287.3/375.7 MB 4.6 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 288.9/375.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------------------------------ --------- 290.2/375.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------------------------------- -------- 291.2/375.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------------------------------- -------- 292.0/375.7 MB 4.6 MB/s eta 0:00:19\n",
      "   ------------------------------- -------- 292.8/375.7 MB 4.6 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 293.6/375.7 MB 4.6 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 294.6/375.7 MB 4.6 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 295.4/375.7 MB 4.6 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 296.7/375.7 MB 4.6 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 298.1/375.7 MB 4.6 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 299.4/375.7 MB 4.6 MB/s eta 0:00:17\n",
      "   ------------------------------- -------- 300.2/375.7 MB 4.6 MB/s eta 0:00:17\n",
      "   -------------------------------- ------- 300.7/375.7 MB 4.6 MB/s eta 0:00:17\n",
      "   -------------------------------- ------- 301.2/375.7 MB 4.6 MB/s eta 0:00:17\n",
      "   -------------------------------- ------- 301.7/375.7 MB 4.6 MB/s eta 0:00:17\n",
      "   -------------------------------- ------- 302.8/375.7 MB 4.6 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 303.6/375.7 MB 4.6 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 304.6/375.7 MB 4.6 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 305.7/375.7 MB 4.6 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 307.2/375.7 MB 4.6 MB/s eta 0:00:15\n",
      "   -------------------------------- ------- 309.1/375.7 MB 4.6 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 310.1/375.7 MB 4.7 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 310.9/375.7 MB 4.7 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 311.2/375.7 MB 4.7 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 311.4/375.7 MB 4.7 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 312.2/375.7 MB 4.7 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 313.0/375.7 MB 4.6 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 313.8/375.7 MB 4.6 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 314.8/375.7 MB 4.6 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 316.1/375.7 MB 4.6 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 317.5/375.7 MB 4.6 MB/s eta 0:00:13\n",
      "   --------------------------------- ------ 319.3/375.7 MB 4.6 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 320.6/375.7 MB 4.6 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 321.1/375.7 MB 4.6 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 321.9/375.7 MB 4.6 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 322.4/375.7 MB 4.6 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 323.0/375.7 MB 4.6 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 323.7/375.7 MB 4.6 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 324.8/375.7 MB 4.6 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 325.8/375.7 MB 4.6 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 327.4/375.7 MB 4.6 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 328.7/375.7 MB 4.7 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 329.8/375.7 MB 4.7 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 330.8/375.7 MB 4.7 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 331.9/375.7 MB 4.7 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 332.7/375.7 MB 4.8 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 333.7/375.7 MB 4.8 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 334.5/375.7 MB 4.8 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 335.3/375.7 MB 4.8 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 336.1/375.7 MB 4.7 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 337.1/375.7 MB 4.7 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 338.2/375.7 MB 4.7 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 338.7/375.7 MB 4.7 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 339.2/375.7 MB 4.7 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 339.7/375.7 MB 4.7 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 340.5/375.7 MB 4.7 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 341.6/375.7 MB 4.7 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 342.4/375.7 MB 4.7 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 343.7/375.7 MB 4.7 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 344.5/375.7 MB 4.7 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 345.5/375.7 MB 4.7 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 346.6/375.7 MB 4.7 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 347.9/375.7 MB 4.7 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 349.4/375.7 MB 4.7 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 350.7/375.7 MB 4.7 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 351.3/375.7 MB 4.7 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 351.8/375.7 MB 4.7 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 352.3/375.7 MB 4.7 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 353.1/375.7 MB 4.7 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 354.2/375.7 MB 4.7 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 355.2/375.7 MB 4.7 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 356.5/375.7 MB 4.7 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 357.6/375.7 MB 4.7 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 358.6/375.7 MB 4.7 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 359.7/375.7 MB 4.7 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 360.4/375.7 MB 4.7 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 361.5/375.7 MB 4.7 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 362.0/375.7 MB 4.7 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 362.8/375.7 MB 4.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 363.9/375.7 MB 4.7 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 365.2/375.7 MB 4.7 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 366.0/375.7 MB 4.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  366.7/375.7 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------------------------------  367.5/375.7 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------------------------------  368.3/375.7 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------------------------------  369.1/375.7 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------------------------------  369.6/375.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  370.1/375.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  370.9/375.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  372.0/375.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  373.0/375.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  374.6/375.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.7/375.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.7/375.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  375.7/375.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 375.7/375.7 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.73.0-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.6/4.3 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 7.9 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.1-cp310-cp310-win_amd64.whl (209 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.6/5.5 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.4/5.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.1/5.5 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.5/5.5 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 4.6 MB/s eta 0:00:00\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.14.0-cp310-cp310-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.3/2.9 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.4/2.9 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.0/1.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 3.4 MB/s eta 0:00:00\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 0.8/1.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.9 MB/s eta 0:00:00\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading wrapt-1.17.2-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp310-cp310-win_amd64.whl (304 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "\n",
      "   - --------------------------------------  1/24 [libclang]\n",
      "   - --------------------------------------  1/24 [libclang]\n",
      "   ------ ---------------------------------  4/24 [werkzeug]\n",
      "   ------ ---------------------------------  4/24 [werkzeug]\n",
      "   ------------- --------------------------  8/24 [optree]\n",
      "   --------------- ------------------------  9/24 [opt-einsum]\n",
      "   -------------------- ------------------- 12/24 [markdown]\n",
      "   --------------------- ------------------ 13/24 [h5py]\n",
      "   --------------------- ------------------ 13/24 [h5py]\n",
      "   ----------------------- ---------------- 14/24 [grpcio]\n",
      "   ------------------------- -------------- 15/24 [google-pasta]\n",
      "   ------------------------------ --------- 18/24 [absl-py]\n",
      "   ------------------------------- -------- 19/24 [tensorboard]\n",
      "   ------------------------------- -------- 19/24 [tensorboard]\n",
      "   ------------------------------- -------- 19/24 [tensorboard]\n",
      "   ------------------------------- -------- 19/24 [tensorboard]\n",
      "   ------------------------------- -------- 19/24 [tensorboard]\n",
      "   --------------------------------- ------ 20/24 [markdown-it-py]\n",
      "   ----------------------------------- ---- 21/24 [rich]\n",
      "   ----------------------------------- ---- 21/24 [rich]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   ------------------------------------ --- 22/24 [keras]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   -------------------------------------- - 23/24 [tensorflow]\n",
      "   ---------------------------------------- 24/24 [tensorflow]\n",
      "\n",
      "Successfully installed absl-py-2.3.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.0 h5py-3.14.0 keras-3.10.0 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 opt-einsum-3.4.0 optree-0.16.0 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b3fe5c2-3440-484b-bb06-cd4e931a304b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dd936a1-16c3-416b-ab82-6ce150b828c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf135ce0-632d-46f7-84ca-5e48415cf240",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289d6b42-4c27-4765-8aa3-fe4375a01f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_1_5B = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 1600,         # Embedding dimension\n",
    "    \"n_heads\": 25,           # Number of attention heads\n",
    "    \"n_layers\": 48,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "925852dd-6793-4724-a7ff-567f266150e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layernorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = 1e-5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1 , keepdim = True)\n",
    "        var = x.var(dim = -1 , keepdim = True , unbiased = False)\n",
    "        norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm + self.shift\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4369cb3e-7955-49c7-8936-357c5484aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e34ef66-7fee-4934-96ee-98ec71160283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"] , 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"] , cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb855ee-b031-4b93-bd93-84d63e396fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout, context_length, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads == 0)\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal = 1))\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.w_key(x)\n",
    "        queries = self.w_query(x)\n",
    "        values = self.w_value(x)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        attn_score = queries @ keys.transpose(2,3)\n",
    "        masked = attn_score.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim = -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeaec6f3-e85e-4104-9abd-c2969ab0edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = Layernorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = Layernorm(cfg[\"emb_dim\"])\n",
    "        self.drop = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29b57cd3-5aa2-43c5-a433-01ac8f428d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range (cfg[\"n_layers\"])])\n",
    "        self.final_norm = Layernorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias = False)\n",
    "    def forward(self, input):\n",
    "        batch_size, seq_len = input.shape\n",
    "        tok_embeds = self.tok_emb(input)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = input.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d059b73-8055-4a70-ba3e-1be33d902844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature = 0.0, top_k = None, eos_id = None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            top_logits,_ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim = -1, keepdim = True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim = 1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f6e010f-6abb-4c5e-a6d2-c7dd442b15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download3 import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21a62aee-c2aa-4405-8e5e-00429859d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size = \"124M\", models_dir = \"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01cfc2ce-9ba7-4815-8995-a1e8a7599e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settings:  {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter keys:  dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"settings: \", settings)\n",
    "print(\"Parameter keys: \", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42346757-efec-461e-a6c3-48b92a795f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config = GPT_CONFIG_124M.copy()\n",
    "new_config.update({\"qkv_bias\" : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28a6bbe2-f99b-4b13-81e8-b394acf6ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = Model_1(new_config)\n",
    "torch.save(gpt.state_dict(), \"ai_poetry.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "104ed9b2-155c-4dee-839e-be4bf0d22dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left is None:\n",
    "        raise ValueError(\"assigning parameter that is none\")\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape Mismatch. Left: {left.shape} , Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90f2a2fb-854d-47e5-bab3-a621f40493e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(gpt, params):\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis = -1)\n",
    "        gpt.trf_blocks[b].att.w_query.weight = assign(gpt.trf_blocks[b].att.w_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.w_key.weight = assign(gpt.trf_blocks[b].att.w_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.w_value.weight = assign(gpt.trf_blocks[b].att.w_value.weight, v_w.T)\n",
    "        \n",
    "        q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis = -1)\n",
    "        gpt.trf_blocks[b].att.w_query.bias = assign(gpt.trf_blocks[b].att.w_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.w_key.bias = assign(gpt.trf_blocks[b].att.w_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.w_value.bias = assign(gpt.trf_blocks[b].att.w_value.bias, v_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(gpt.trf_blocks[b].att.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(gpt.trf_blocks[b].att.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(gpt.trf_blocks[b].ff.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(gpt.trf_blocks[b].ff.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(gpt.trf_blocks[b].ff.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "        \n",
    "        gpt.trf_blocks[b].norm1.scale = assign(gpt.trf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(gpt.trf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(gpt.trf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(gpt.trf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "    \n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign( gpt.out_head.weight, params[\"wte\"])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3114e376-9a3e-4f8d-b17f-a1b98f16245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fdad2df-8a42-4fcd-ac79-fc4075c1e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db40ff2a-9e80-407f-95e8-57233fbbc04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_1(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): Layernorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "555d1cdd-0721-478d-a0fe-15fa47285798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (74728, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Original datasets\n",
    "df1 = pd.read_csv(\"poetry_foundation_poems.csv\")\n",
    "df2 = pd.read_csv(\"poki.csv\")\n",
    "\n",
    "# New datasets\n",
    "df3 = pd.read_csv(\"shakespeare_poems.csv\", header=None, names=[\"text\"], on_bad_lines='skip')\n",
    "\n",
    "df4 = pd.read_csv(\"romantic_poets.csv\")\n",
    "\n",
    "df5 = pd.read_csv(\"PoetryFoundationData.csv\")  # new extended dataset\n",
    "df6 = pd.read_csv(\"all.csv\") \n",
    "\n",
    "# Rename columns\n",
    "df1 = df1.rename(columns={\"Poem\": \"text\"})\n",
    "df2 = df2.rename(columns={\"Poem\": \"text\"})\n",
    "df3 = df3.rename(columns={\"Poem\": \"text\"})\n",
    "df4 = df4.rename(columns={\"poem content\": \"text\"})\n",
    "df5 = df5.rename(columns={\"Poem\": \"text\"})\n",
    "df6 = df6.rename(columns={\"content\": \"text\"})\n",
    "\n",
    "# Convert raw text to dataframe\n",
    "df7 = pd.DataFrame({\"text\": [open(\"gutenberg.txt\", \"r\", encoding=\"utf-8\").read()]})\n",
    "\n",
    "\n",
    "# Combine all\n",
    "combined = pd.concat(\n",
    "    [df1[[\"text\"]], df2[[\"text\"]], df3[[\"text\"]], df4[[\"text\"]], df5[[\"text\"]],df6[[\"text\"]], df7[[\"text\"]]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Clean and filter\n",
    "combined.drop_duplicates(subset=\"text\", inplace=True)\n",
    "combined = combined[combined[\"text\"].str.len() > 50].reset_index(drop=True)\n",
    "\n",
    "print(\"Combined dataset shape:\", combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba960a-217d-48ce-91f6-43b102a65475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "028c6200-45a1-4518-b793-4a3f02befea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "combined['text'] = combined['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c15a20a0-d85e-4bde-a312-bae51e9fd99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"combined_poems_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "442bf061-f0cc-4013-8b51-a089f7d91e1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"she makes the world happy and full of joy. she give little kids one of her special toy i think she makes me bloom. ''you are too considerate'' i assume. when we do our homework she makes me smart but when im lonely i feel like tart but that's why she's my friend and she will always have that beautiful trend.\"\n",
      " 'life life is like a broken bone life doesnâ’t always have happy endings people are nice and people are mean life is like a broken bone sometimes you get you want sometimes you donâ’t life is like a broken bone mom is yelling and dad is screaming people are always yelling at you and most of the time you are confused by: breanna'\n",
      " 'those are my drums black, white, and several tumbs. when played people respond as i kick, beat those beautiful tumbs. if i choose to stop i cannot bear it but, i will keep on going until my life is cut.'\n",
      " 'pie oh pie i love pie so everytime you see pie be sure to say hi to pie.'\n",
      " \"NO more of talk where God or Angel Guest With Man, as with his Friend, familiar us'd To sit indulgent, and with him partake Rural repast, permitting him the while Venial discourse unblam'd: I now must change Those Notes to Tragic; foul distrust, and breach Disloyal on the part of Man, revolt, And disobedience: On the part of Heav'n Now alienated, distance and distaste, Anger and just rebuke, and judgement giv'n, That brought into this World a world of woe, Sinne and her shadow Death, and Miserie Deaths Harbinger: Sad task, yet argument Not less but more Heroic then the wrauth Of stern Achilles on his Foe pursu'd Thrice Fugitive about Troy Wall; or rage Of Turnus for Lavinia disespous'd, Or Neptun's ire or Juno's, that so long Perplex'd the Greek and Cytherea's Son; If answerable style I can obtaine Of my Celestial Patroness, who deignes Her nightly visitation unimplor'd, And dictates to me slumbring, or inspires Easie my unpremeditated Verse: Since first this Subject for Heroic Song Pleas'd me long choosing, and beginning late; Not sedulous by Nature to indite Warrs, hitherto the onely Argument Heroic deem'd, chief maistrie to dissect With long and tedious havoc fabl'd Knights In Battels feign'd; the better fortitude Of Patience and Heroic Martyrdom Unsung; or to describe Races and Games, Or tilting Furniture, emblazon'd Shields, Impreses quaint, Caparisons and Steeds; Bases and tinsel Trappings, gorgious Knights At Joust and Torneament; then marshal'd Feast Serv'd up in Hall with Sewers, and Seneshals; The skill of Artifice or Office mean, Not that which justly gives Heroic name To Person or to Poem. Mee of these Nor skilld nor studious, higher Argument Remaines, sufficient of it self to raise That name, unless an age too late, or cold Climat, or Years damp my intended wing Deprest, and much they may, if all be mine, Not Hers who brings it nightly to my Ear. The Sun was sunk, and after him the Starr Of Hesperus, whose Office is to bring Twilight upon the Earth, short Arbiter Twixt Day and Night, and now from end to end Nights Hemisphere had veild the Horizon round: When Satan who late fled before the threats Of Gabriel out of Eden, now improv'd In meditated fraud and malice, bent On mans destruction, maugre what might hap Of heavier on himself, fearless return'd. By Night he fled, and at Midnight return'd From compassing the Earth, cautious of day, Since Uriel Regent of the Sun descri'd His entrance, and forewarnd the Cherubim That kept thir watch; thence full of anguish driv'n, The space of seven continu'd Nights he rode With darkness, thrice the Equinoctial Line He circl'd, four times cross'd the Carr of Night From Pole to Pole, traversing each Colure; On the eighth return'd, and on the Coast averse From entrance or Cherubic Watch, by stealth Found unsuspected way. There was a place, Now not, though Sin, not Time, first wraught the change, Where Tigris at the foot of Paradise Into a Gulf shot under ground, till part Rose up a Fountain by the Tree of Life; In with the River sunk, and with it rose Satan involv'd in rising Mist, then sought Where to lie hid; Sea he had searcht and Land From Eden over Pontus, and the PooleMaotis, up beyond the River Ob; Downward as farr Antartic; and in length West from Orontes to the Ocean barr'd At Darien, thence to the Land where flowesGanges and Indus: thus the Orb he roam'd With narrow search; and with inspection deep Consider'd every Creature, which of all Most opportune might serve his Wiles, and found The Serpent suttlest Beast of all the Field. Him after long debate, irresolute Of thoughts revolv'd, his final sentence chose Fit Vessel, fittest Imp of fraud, in whom To enter, and his dark suggestions hide From sharpest sight: for in the wilie Snake, Whatever sleights none would suspicious mark, As from his wit and native suttletie Proceeding, which in other Beasts observ'd Doubt might beget of Diabolic pow'r Active within beyond the sense of brute. Thus he resolv'd, but first from inward griefe His bursting passion into plaints thus pour'd: O Earth, how like to Heav'n, if not preferr'd More justly, Seat worthier of Gods, as built With second thoughts, reforming what was old! For what God after better worse would build? Terrestrial Heav'n, danc't round by other Heav'ns That shine, yet bear thir bright officious Lamps, Light above Light, for thee alone, as seems, In thee concentring all thir precious beams Of sacred influence: As God in Heav'n Is Center, yet extends to all, so thou Centring receav'st from all those Orbs; in thee, Not in themselves, all thir known vertue appeers Productive in Herb, Plant, and nobler birth Of Creatures animate with gradual life Of Growth, Sense, Reason, all summ'd up in Man. With what delight could I have walkt thee round, If I could joy in aught, sweet interchange Of Hill, and Vallie, Rivers, Woods and Plaines, Now Land, now Sea, and Shores with Forrest crownd, Rocks, Dens, and Caves; but I in none of these Find place or refuge; and the more I see Pleasures about me, so much more I feel Torment within me, as from the hateful siege Of contraries; all good to me becomes Bane, and in Heav'n much worse would be my state. But neither here seek I, no nor in Heav'n To dwell, unless by maistring Heav'ns Supreame; Nor hope to be my self less miserable By what I seek, but others to make such As I, though thereby worse to me redound: For onely in destroying I find ease To my relentless thoughts; and him destroyd, Or won to what may work his utter loss, For whom all this was made, all this will soon Follow, as to him linkt in weal or woe, In wo then; that destruction wide may range: To mee shall be the glorie sole among The infernal Powers, in one day to have marr'd What he Almightie styl'd, six Nights and Days Continu'd making, and who knows how long Before had bin contriving, though perhaps Not longer then since I in one Night freed From servitude inglorious welnigh half Th' Angelic Name, and thinner left the throng Of his adorers: hee to be aveng'd, And to repaire his numbers thus impair'd, Whether such vertue spent of old now faild More Angels to Create, if they at least Are his Created, or to spite us more, Determin'd to advance into our room A Creature form'd of Earth, and him endow, Exalted from so base original, With Heav'nly spoils, our spoils: What he decreed He effected; Man he made, and for him built Magnificent this World, and Earth his seat, Him Lord pronounc'd, and, O indignitie! Subjected to his service Angel wings, And flaming Ministers to watch and tend Thir earthy Charge: Of these the vigilance I dread, and to elude, thus wrapt in mist Of midnight vapor glide obscure, and prie In every Bush and Brake, where hap may finde The Serpent sleeping, in whose mazie foulds To hide me, and the dark intent I bring. O foul descent! that I who erst contended With Gods to sit the highest, am now constraind Into a Beast, and mixt with bestial slime, This essence to incarnate and imbrute, That to the hight of Deitie aspir'd; But what will not Ambition and Revenge Descend to? who aspires must down as low As high he soard, obnoxious first or last To basest things. Revenge, at first though sweet, Bitter ere long back on it self recoiles; Let it; I reck not, so it light well aim'd, Since higher I fall short, on him who next Provokes my envie, this new Favorite Of Heav'n, this Man of Clay, Son of despite, Whom us the more to spite his Maker rais'd From dust: spite then with spite is best repaid. So saying, through each Thicket Danck or Drie, Like a black mist low creeping, he held on His midnight search, where soonest he might finde The Serpent: him fast sleeping soon he found In Labyrinth of many a round self-rowld, His head the midst, well stor'd with suttle wiles: Not yet in horrid Shade or dismal Den, Nor nocent yet, but on the grassie Herbe Fearless unfeard he slept: in at his Mouth The Devil enterd, and his brutal sense, In heart or head, possessing soon inspir'd With act intelligential, but his sleep Disturbd not, waiting close th' approach of Morn. Now when as sacred Light began to dawne In Eden on the humid Flours, that breathd Thir morning incense, when all things that breath, From th' Earths great Altar send up silent praise To the Creator, and his Nostrils fill With grateful Smell, forth came the human pair And joind thir vocal Worship to the Quire Of Creatures wanting voice, that done, partake The season, prime for sweetest Sents and Aires: Then commune how that day they best may ply Thir growing work: for much thir work outgrew The hands dispatch of two Gardning so wide. And Eve first to her Husband thus began.Adam, well may we labour still to dress This Garden, still to tend Plant, Herb and Flour, Our pleasant task enjoyn'd, but till more hands Aid us, the work under our labour grows, Luxurious by restraint; what we by day Lop overgrown, or prune, or prop, or bind, One night or two with wanton growth derides Tending to wilde. Thou therefore now advise Or hear what to my minde first thoughts present, Let us divide our labours, thou where choice Leads thee, or where most needs, whether to wind The Woodbine round this Arbour, or direct The clasping Ivie where to climb, while I In yonder Spring of Roses intermixt With Myrtle, find what to redress till Noon: For while so near each other thus all day Our taske we choose, what wonder if so near Looks intervene and smiles, or object new Casual discourse draw on, which intermits Our dayes work brought to little, though begun Early, and th' hour of Supper comes unearn'd. To whom mild answer Adam thus return'd. Sole Eve, Associate sole, to me beyond Compare above all living Creatures deare, Well hast thou motion'd, well thy thoughts imployd How we might best fulfill the work which here God hath assign'd us, nor of me shalt pass Unprais'd: for nothing lovelier can be found In Woman, then to studie houshold good, And good workes in her Husband to promote. Yet not so strictly hath our Lord impos'd Labour, as to debarr us when we need Refreshment, whether food, or talk between, Food of the mind, or this sweet intercourse Of looks and smiles, for smiles from Reason flow, To brute deni'd, and are of Love the food, Love not the lowest end of human life. For not to irksom toile, but to delight He made us, and delight to Reason joyn'd. These paths & Bowers doubt not but our joynt hands Will keep from Wilderness with ease, as wide As we need walk, till younger hands ere long Assist us: But if much converse perhaps Thee satiate, to short absence I could yield. For solitude somtimes is best societie, And short retirement urges sweet returne. But other doubt possesses me, least harm Befall thee sever'd from me; for thou knowst What hath bin warn'd us, what malicious Foe Envying our happiness, and of his own Despairing, seeks to work us woe and shame By sly assault; and somwhere nigh at hand Watches, no doubt, with greedy hope to find His wish and best advantage, us asunder, Hopeless to circumvent us joynd, where each To other speedie aide might lend at need; Whether his first design be to withdraw Our fealtie from God, or to disturb Conjugal Love, then which perhaps no bliss Enjoy'd by us excites his envie more; Or this, or worse, leave not the faithful side That gave thee being, still shades thee and protects. The Wife, where danger or dishonour lurks, Safest and seemliest by her Husband staies, Who guards her, or with her the worst endures. To whom the Virgin Majestie of Eve, As one who loves, and some unkindness meets, With sweet austeer composure thus reply'd, Ofspring of Heav'n and Earth, and all Earths Lord, That such an Enemie we have, who seeks Our ruin, both by thee informd I learne, And from the parting Angel over-heard As in a shadie nook I stood behind, Just then returnd at shut of Evening Flours. But that thou shouldst my firmness therfore doubt To God or thee, because we have a foe May tempt it, I expected not to hear. His violence thou fearst not, being such, As wee, not capable of death or paine, Can either not receave, or can repell. His fraud is then thy fear, which plain inferrs Thy equal fear that my firm Faith and Love Can by his fraud be shak'n or seduc't; Thoughts, which how found they harbour in thy brestAdam, missthought of her to thee so dear? To whom with healing words Adam replyd. Daughter of God and Man, immortal Eve, For such thou art, from sin and blame entire: Not diffident of thee do I dissuade Thy absence from my sight, but to avoid Th' attempt it self, intended by our Foe. For hee who tempts, though in vain, at least asperses The tempted with dishonour foul, suppos'd Not incorruptible of Faith, not prooff Against temptation: thou thy self with scorne And anger wouldst resent the offer'd wrong, Though ineffectual found: misdeem not then, If such affront I labour to avert From thee alone, which on us both at once The Enemie, though bold, will hardly dare, Or daring, first on mee th' assault shall light. Nor thou his malice and false guile contemn; Suttle he needs must be, who could seduce Angels, nor think superfluous others aid. I from the influence of thy looks receave Access in every Vertue, in thy sight More wise, more watchful, stronger, if need were Of outward strength; while shame, thou looking on, Shame to be overcome or over-reacht Would utmost vigor raise, and rais'd unite. Why shouldst not thou like sense within thee feel When I am present, and thy trial choose With me, best witness of thy Vertue tri'd. So spake domestick Adam in his care And Matrimonial Love; but Eve, who thought Less attributed to her Faith sincere, Thus her reply with accent sweet renewd. If this be our condition, thus to dwell In narrow circuit strait'nd by a Foe, Suttle or violent, we not endu'd Single with like defence, wherever met, How are we happie, still in fear of harm? But harm precedes not sin: onely our Foe Tempting affronts us with his foul esteem Of our integritie: his foul esteeme Sticks no dishonour on our Front, but turns Foul on himself; then wherefore shund or feard By us? who rather double honour gaine From his surmise prov'd false, find peace within, Favour from Heav'n, our witness from th' event. And what is Faith, Love, Vertue unassaid Alone, without exterior help sustaind? Let us not then suspect our happie State Left so imperfet by the Maker wise, As not secure to single or combin'd. Fraile is our happiness, if this be so, And Eden were no Eden thus expos'd. To whom thus Adam fervently repli'd. O Woman, best are all things as the will Of God ordain'd them, his creating hand Nothing imperfet or deficient left Of all that he Created, much less Man, Or aught that might his happie State secure, Secure from outward force; within himself The danger lies, yet lies within his power: Against his will he can receave no harme. But God left free the Will, for what obeyes Reason, is free, and Reason he made right, But bid her well beware, and still erect, Least by some faire appeering good surpris'd She dictate false, and misinforme the Will To do what God expressly hath forbid. Not then mistrust, but tender love enjoynes, That I should mind thee oft, and mind thou me. Firm we subsist, yet possible to swerve, Since Reason not impossibly may meet Some specious object by the Foe subornd, And fall into deception unaware, Not keeping strictest watch, as she was warnd. Seek not temptation then, which to avoide Were better, and most likelie if from mee Thou sever not: Trial will come unsought. Wouldst thou approve thy constancie, approve First thy obedience; th' other who can know, Not seeing thee attempted, who attest? But if thou think, trial unsought may finde Us both securer then thus warnd thou seemst, Go; for thy stay, not free, absents thee more; Go in thy native innocence, relie On what thou hast of vertue, summon all, For God towards thee hath done his part, do thine. So spake the Patriarch of Mankinde, but Eve Persisted, yet submiss, though last, repli'd. With thy permission then, and thus forewarnd Chiefly by what thy own last reasoning words Touchd onely, that our trial, when least sought, May finde us both perhaps farr less prepar'd, The willinger I goe, nor much expect A Foe so proud will first the weaker seek; So bent, the more shall shame him his repulse. Thus saying, from her Husbands hand her hand Soft she withdrew, and like a Wood-Nymph lightOread or Dryad, or of Delia's Traine, Betook her to the Groves, but Delia's self In gate surpass'd and Goddess-like deport, Though not as shee with Bow and Quiver armd, But with such Gardning Tools as Art yet rude, Guiltless of fire had formd, or Angels brought. To Pales, or Pomona thus adornd, Likeliest she seemd, Pomona when she fledVertumnus, or to Ceres in her Prime, Yet Virgin of Proserpina from Jove. Her long with ardent look his Eye pursu'd Delighted, but desiring more her stay. Oft he to her his charge of quick returne Repeated, shee to him as oft engag'd To be returnd by Noon amid the Bowre, And all things in best order to invite Noontide repast, or Afternoons repose. O much deceav'd, much failing, hapless Eve, Of thy presum'd return! event perverse! Thou never from that houre in Paradise Foundst either sweet repast, or sound repose; Such ambush hid among sweet Flours and Shades Waited with hellish rancour imminent To intercept thy way, or send thee back Despoild of Innocence, of Faith, of Bliss. For now, and since first break of dawne the Fiend, Meer Serpent in appearance, forth was come, And on his Quest, where likeliest he might finde The onely two of Mankinde, but in them The whole included Race, his purposd prey. In Bowre and Field he sought, where any tuft Of Grove or Garden-Plot more pleasant lay, Thir tendance or Plantation for delight, By Fountain or by shadie Rivulet He sought them both, but wish'd his hap might findEve separate, he wish'd, but not with hope Of what so seldom chanc'd, when to his wish, Beyond his hope, Eve separate he spies, Veild in a Cloud of Fragrance, where she stood, Half spi'd, so thick the Roses bushing round About her glowd, oft stooping to support Each Flour of slender stalk, whose head though gay Carnation, Purple, Azure, or spect with Gold, Hung drooping unsustaind, them she upstaies Gently with Mirtle band, mindless the while, Her self, though fairest unsupported Flour, From her best prop so farr, and storm so nigh. Neerer he drew, and many a walk travers'd Of stateliest Covert, Cedar, Pine, or Palme, Then voluble and bold, now hid, now seen Among thick-wov'n Arborets and Flours Imborderd on each Bank, the hand of Eve: Spot more delicious then those Gardens feign'd Or of reviv'd Adonis, or renowndAlcinous, host of old Laertes Son, Or that, not Mystic, where the Sapient King Held dalliance with his faire Egyptian Spouse. Much hee the Place admir'd, the Person more. As one who long in populous City pent, Where Houses thick and Sewers annoy the Aire, Forth issuing on a Summers Morn to breathe Among the pleasant Villages and Farmes Adjoynd, from each thing met conceaves delight, The smell of Grain, or tedded Grass, or Kine, Or Dairie, each rural sight, each rural sound; If chance with Nymphlike step fair Virgin pass, What pleasing seemd, for her now pleases more, She most, and in her look summs all Delight. Such Pleasure took the Serpent to behold This Flourie Plat, the sweet recess of Eve Thus earlie, thus alone; her Heav'nly forme Angelic, but more soft, and Feminine, Her graceful Innocence, her every Aire Of gesture or lest action overawd His Malice, and with rapine sweet bereav'd His fierceness of the fierce intent it brought: That space the Evil one abstracted stood From his own evil, and for the time remaind Stupidly good, of enmitie disarm'd, Of guile, of hate, of envie, of revenge; But the hot Hell that alwayes in him burnes, Though in mid Heav'n, soon ended his delight, And tortures him now more, the more he sees Of pleasure not for him ordain'd: then soon Fierce hate he recollects, and all his thoughts Of mischief, gratulating, thus excites. Thoughts, whither have ye led me, with what sweet Compulsion thus transported to forget What hither brought us, hate, not love, nor hope Of Paradise for Hell, hope here to taste Of pleasure, but all pleasure to destroy, Save what is in destroying, other joy To me is lost. Then let me not let pass Occasion which now smiles, behold alone The Woman, opportune to all attempts, Her Husband, for I view far round, not nigh, Whose higher intellectual more I shun, And strength, of courage hautie, and of limb Heroic built, though of terrestrial mould, Foe not informidable, exempt from wound, I not; so much hath Hell debas'd, and paine Infeebl'd me, to what I was in Heav'n. Shee fair, divinely fair, fit Love for Gods, Not terrible, though terrour be in Love And beautie, not approacht by stronger hate, Hate stronger, under shew of Love well feign'd, The way which to her ruin now I tend. So spake the Enemie of Mankind, enclos'd In Serpent, Inmate bad, and toward Eve Address'd his way, not with indented wave, Prone on the ground, as since, but on his reare, Circular base of rising foulds, that tour'd Fould above fould a surging Maze, his Head Crested aloft, and Carbuncle his Eyes; With burnisht Neck of verdant Gold, erect Amidst his circling Spires, that on the grass Floted redundant: pleasing was his shape, And lovely, never since of Serpent kind Lovelier, not those that in Illyria chang'dHermione and Cadmus, or the God In Epidaurus; nor to which transformdAmmonian Jove, or Capitoline was seen, Hee with Olympias, this with her who boreScipio the highth of Rome. With tract oblique At first, as one who sought access, but feard To interrupt, side-long he works his way. As when a Ship by skilful Stearsman wrought Nigh Rivers mouth or Foreland, where the Wind Veres oft, as oft so steers, and shifts her Saile; So varied hee, and of his tortuous Traine Curld many a wanton wreath in sight of Eve, To lure her Eye; shee busied heard the sound Of rusling Leaves, but minded not, as us'd To such disport before her through the Field, From every Beast, more duteous at her call, Then at Circean call the Herd disguis'd. Hee boulder now, uncall'd before her stood; But as in gaze admiring: Oft he bowd His turret Crest, and sleek enamel'd Neck, Fawning, and lick'd the ground whereon she trod. His gentle dumb expression turnd at length The Eye of Eve to mark his play; he glad Of her attention gaind, with Serpent Tongue Organic, or impulse of vocal Air, His fraudulent temptation thus began. Wonder not, sovran Mistress, if perhaps Thou canst, who art sole Wonder, much less arm Thy looks, the Heav'n of mildness, with disdain, Displeas'd that I approach thee thus, and gaze Insatiate, I thus single, nor have feard Thy awful brow, more awful thus retir'd. Fairest resemblance of thy Maker faire, Thee all things living gaze on, all things thine By gift, and thy Celestial Beautie adore With ravishment beheld, there best beheld Where universally admir'd; but here In this enclosure wild, these Beasts among, Beholders rude, and shallow to discerne Half what in thee is fair, one man except, Who sees thee? (and what is one?) who shouldst be seen A Goddess among Gods, ador'd and serv'd By Angels numberless, thy daily Train. So gloz'd the Tempter, and his Proem tun'd; Into the Heart of Eve his words made way, Though at the voice much marveling; at length Not unamaz'd she thus in answer spake. What may this mean? Language of Man pronounc't By Tongue of Brute, and human sense exprest? The first at lest of these I thought deni'd To Beasts, whom God on thir Creation-Day Created mute to all articulat sound; The latter I demurre, for in thir looks Much reason, and in thir actions oft appeers. Thee, Serpent, suttlest beast of all the field I knew, but not with human voice endu'd; Redouble then this miracle, and say, How cam'st thou speakable of mute, and how To me so friendly grown above the rest Of brutal kind, that daily are in sight? Say, for such wonder claims attention due. To whom the guileful Tempter thus reply'd. Empress of this fair World, resplendent Eve, Easie to mee it is to tell thee all What thou commandst, and right thou shouldst be obeyd: I was at first as other Beasts that graze The trodden Herb, of abject thoughts and low, As was my food, nor aught but food discern'd Or Sex, and apprehended nothing high: Till on a day roaving the field, I chanc'd A goodly Tree farr distant to behold Loaden with fruit of fairest colours mixt, Ruddie and Gold: I nearer drew to gaze; When from the boughes a savorie odour blow'n, Grateful to appetite, more pleas'd my sense Then smell of sweetest Fenel or the Teats Of Ewe or Goat dropping with Milk at Eevn, Unsuckt of Lamb or Kid, that tend thir play. To satisfie the sharp desire I had Of tasting those fair Apples, I resolv'd Not to deferr; hunger and thirst at once, Powerful perswaders, quick'nd at the scent Of that alluring fruit, urg'd me so keene. About the mossie Trunk I wound me soon, For high from ground the branches would require Thy utmost reach or Adams: Round the Tree All other Beasts that saw, with like desire Longing and envying stood, but could not reach. Amid the Tree now got, where plenty hung Tempting so nigh, to pluck and eat my fill I spar'd not, for such pleasure till that hour At Feed or Fountain never had I found. Sated at length, ere long I might perceave Strange alteration in me, to degree Of Reason in my inward Powers, and Speech Wanted not long, though to this shape retain'd. Thenceforth to Speculations high or deep I turnd my thoughts, and with capacious mind Considerd all things visible in Heav'n, Or Earth, or Middle, all things fair and good; But all that fair and good in thy Divine Semblance, and in thy Beauties heav'nly Ray United I beheld; no Fair to thine Equivalent or second, which compel'd Mee thus, though importune perhaps, to come And gaze, and worship thee of right declar'd Sovran of Creatures, universal Dame. So talk'd the spirited sly Snake; and Eve Yet more amaz'd unwarie thus reply'd. Serpent, thy overpraising leaves in doubt The vertue of that Fruit, in thee first prov'd: But say, where grows the Tree, from hence how far? For many are the Trees of God that grow In Paradise, and various, yet unknown To us, in such aboundance lies our choice, As leaves a greater store of Fruit untoucht, Still hanging incorruptible, till men Grow up to thir provision, and more hands Help to disburden Nature of her Bearth. To whom the wilie Adder, blithe and glad. Empress, the way is readie, and not long, Beyond a row of Myrtles, on a Flat, Fast by a Fountain, one small Thicket past Of blowing Myrrh and Balme; if thou accept My conduct, I can bring thee thither soon. Lead then, said Eve. Hee leading swiftly rowld In tangles, and made intricate seem strait, To mischief swift. Hope elevates, and joy Bright'ns his Crest, as when a wandring Fire, Compact of unctuous vapor, which the Night Condenses, and the cold invirons round, Kindl'd through agitation to a Flame, Which oft, they say, some evil Spirit attends Hovering and blazing with delusive Light, Misleads th' amaz'd Night-wanderer from his way To Boggs and Mires, and oft through Pond or Poole, There swallow'd up and lost, from succour farr. So glister'd the dire Snake, and into fraud Led Eve our credulous Mother, to the Tree Of prohibition, root of all our woe; Which when she saw, thus to her guide she spake. Serpent, we might have spar'd our coming hither, Fruitless to mee, though Fruit be here to excess, The credit of whose vertue rest with thee, Wondrous indeed, if cause of such effects. But of this Tree we may not taste nor touch; God so commanded, and left that Command Sole Daughter of his voice; the rest, we live Law to our selves, our Reason is our Law. To whom the Tempter guilefully repli'd. Indeed? hath God then said that of the Fruit Of all these Garden Trees ye shall not eate, Yet Lords declar'd of all in Earth or Aire? To whom thus Eve yet sinless. Of the Fruit Of each Tree in the Garden we may eate, But of the Fruit of this fair Tree amidst The Garden, God hath said, Ye shall not eate Thereof, nor shall ye touch it, least ye die. She scarse had said, though brief, when now more bold The Tempter, but with shew of Zeale and Love To Man, and indignation at his wrong, New part puts on, and as to passion mov'd, Fluctuats disturbd, yet comely and in act Rais'd, as of som great matter to begin. As when of old som Orator renound In Athens or free Rome, where Eloquence Flourishd, since mute, to som great cause addrest, Stood in himself collected, while each part, Motion, each act won audience ere the tongue, Somtimes in highth began, as no delay Of Preface brooking through his Zeal of Right. So standing, moving, or to highth upgrown The Tempter all impassiond thus began. O Sacred, Wise, and Wisdom-giving Plant, Mother of Science, Now I feel thy Power Within me cleere, not onely to discerne Things in thir Causes, but to trace the wayes Of highest Agents, deemd however wise. Queen of this Universe, doe not believe Those rigid threats of Death; ye shall not Die: How should ye? by the Fruit? it gives you Life To Knowledge? By the Threatner, look on mee, Mee who have touch'd and tasted, yet both live, And life more perfet have attaind then Fate Meant mee, by ventring higher then my Lot. Shall that be shut to Man, which to the Beast Is open? or will God incense his ire For such a petty Trespass, and not praise Rather your dauntless vertue, whom the pain Of Death denounc't, whatever thing Death be, Deterrd not from atchieving what might leade To happier life, knowledge of Good and Evil; Of good, how just? of evil, if what is evil Be real, why not known, since easier shunnd? God therefore cannot hurt ye, and be just; Not just, not God; not feard then, nor obeyd: Your feare it self of Death removes the feare. Why then was this forbid? Why but to awe, Why but to keep ye low and ignorant, His worshippers; he knows that in the day Ye Eate thereof, your Eyes that seem so cleere, Yet are but dim, shall perfetly be then Op'nd and cleerd, and ye shall be as Gods, Knowing both Good and Evil as they know. That ye should be as Gods, since I as Man, Internal Man, is but proportion meet, I of brute human, yee of human Gods. So ye shall die perhaps, by putting off Human, to put on Gods, death to be wisht, Though threat'nd, which no worse then this can bring. And what are Gods that Man may not become As they, participating God-like food? The Gods are first, and that advantage use On our belief, that all from them proceeds; I question it, for this fair Earth I see, Warm'd by the Sun, producing every kind, Them nothing: If they all things, who enclos'd Knowledge of Good and Evil in this Tree, That whoso eats thereof, forthwith attains Wisdom without their leave? and wherein lies Th' offence, that Man should thus attain to know? What can your knowledge hurt him, or this Tree Impart against his will if all be his? Or is it envie, and can envie dwell In heav'nly breasts? these, these and many more Causes import your need of this fair Fruit. Goddess humane, reach then, and freely taste. He ended, and his words replete with guile Into her heart too easie entrance won: Fixt on the Fruit she gaz'd, which to behold Might tempt alone, and in her ears the sound Yet rung of his perswasive words, impregn'd With Reason, to her seeming, and with Truth; Mean while the hour of Noon drew on, and wak'd An eager appetite, rais'd by the smell So savorie of that Fruit, which with desire, Inclinable now grown to touch or taste, Sollicited her longing eye; yet first Pausing a while, thus to her self she mus'd. Great are thy Vertues, doubtless, best of Fruits, Though kept from Man, and worthy to be admir'd, Whose taste, too long forborn, at first assay Gave elocution to the mute, and taught The Tongue not made for Speech to speak thy praise: Thy praise hee also who forbids thy use, Conceales not from us, naming thee the Tree Of Knowledge, knowledge both of good and evil; Forbids us then to taste, but his forbidding Commends thee more, while it inferrs the good By thee communicated, and our want: For good unknown, sure is not had, or had And yet unknown, is as not had at all. In plain then, what forbids he but to know, Forbids us good, forbids us to be wise? Such prohibitions binde not. But if Death Bind us with after-bands, what profits then Our inward freedom? In the day we eate Of this fair Fruit, our doom is, we shall die. How dies the Serpent? hee hath eat'n and lives, And knows, and speaks, and reasons, and discerns, Irrational till then. For us alone Was death invented? or to us deni'd This intellectual food, for beasts reserv'd? For Beasts it seems: yet that one Beast which first Hath tasted, envies not, but brings with joy The good befall'n him, Author unsuspect, Friendly to man, farr from deceit or guile. What fear I then, rather what know to feare Under this ignorance of good and Evil, Of God or Death, of Law or Penaltie? Here grows the Cure of all, this Fruit Divine, Fair to the Eye, inviting to the Taste, Of vertue to make wise: what hinders then To reach, and feed at once both Bodie and Mind? So saying, her rash hand in evil hour Forth reaching to the Fruit, she pluck'd, she eat: Earth felt the wound, and Nature from her seat Sighing through all her Works gave signs of woe, That all was lost. Back to the Thicket slunk The guiltie Serpent, and well might, for Eve Intent now wholly on her taste, naught else Regarded, such delight till then, as seemd, In Fruit she never tasted, whether true Or fansied so, through expectation high Of knowledg, nor was God-head from her thought. Greedily she ingorg'd without restraint, And knew not eating Death: Satiate at length, And hight'nd as with Wine, jocond and boon, Thus to her self she pleasingly began. O Sovran, vertuous, precious of all Trees In Paradise, of operation blest To Sapience, hitherto obscur'd, infam'd, And thy fair Fruit let hang, as to no end Created; but henceforth my early care, Not without Song, each Morning, and due praise Shall tend thee, and the fertil burden ease Of thy full branches offer'd free to all; Till dieted by thee I grow mature In knowledge, as the Gods who all things know; Though others envie what they cannot give; For had the gift bin theirs, it had not here Thus grown. Experience, next to thee I owe, Best guide; not following thee, I had remaind In ignorance, thou op'nst Wisdoms way, And giv'st access, though secret she retire. And I perhaps am secret; Heav'n is high, High and remote to see from thence distinct Each thing on Earth; and other care perhaps May have diverted from continual watch Our great Forbidder, safe with all his Spies About him. But to Adam in what sort Shall I appeer? shall I to him make known As yet my change, and give him to partake Full happiness with mee, or rather not, But keep the odds of Knowledge in my power Without Copartner? so to add what wants In Femal Sex, the more to draw his Love, And render me more equal, and perhaps, A thing not undesireable, somtime Superior; for inferior who is free? This may be well: but what if God have seen, And Death ensue? then I shall be no more, And Adam wedded to another Eve, Shall live with her enjoying, I extinct; A death to think. Confirm'd then I resolve;Adam shall share with me in bliss or woe: So dear I love him, that with him all deaths I could endure, without him live no life. So saying, from the Tree her step she turnd, But first low Reverence don, as to the power That dwelt within, whose presence had infus'd Into the plant sciential sap, deriv'd From Nectar, drink of Gods. Adam the while Waiting desirous her return, had wove Of choicest Flours a Garland to adorne Her Tresses, and her rural labours crown, As Reapers oft are wont thir Harvest Queen. Great joy he promis'd to his thoughts, and new Solace in her return, so long delay'd; Yet oft his heart, divine of somthing ill, Misgave him; hee the faultring measure felt; And forth to meet her went, the way she took That Morn when first they parted; by the Tree Of Knowledge he must pass, there he her met, Scarse from the Tree returning; in her hand A bough of fairest fruit that downie smil'd, New gatherd, and ambrosial smell diffus'd. To him she hasted, in her face excuse Came Prologue, and Apologie to prompt, Which with bland words at will she thus addrest. Hast thou not wonderd, Adam, at my stay? Thee I have misst, and thought it long, depriv'd Thy presence, agonie of love till now Not felt, nor shall be twice, for never more Mean I to trie, what rash untri'd I sought, The pain of absence from thy sight. But strange Hath bin the cause, and wonderful to heare: This Tree is not as we are told, a Tree Of danger tasted, nor to evil unknown Op'ning the way, but of Divine effect To open Eyes, and make them Gods who taste; And hath bin tasted such: the Serpent wise, Or not restraind as wee, or not obeying, Hath eat'n of the fruit, and is become, Not dead, as we are threatn'd, but thenceforth Endu'd with human voice and human sense, Reasoning to admiration, and with mee Perswasively hath so prevaild, that I Have also tasted, and have also found Th' effects to correspond, opener mine Eyes, Dimm erst, dilated Spirits, ampler Heart, And growing up to Godhead; which for thee Chiefly I sought, without thee can despise. For bliss, as thou hast part, to me is bliss, Tedious, unshar'd with thee, and odious soon. Thou therefore also taste, that equal Lot May joyne us, equal joy, as equal Love; Least thou not tasting, different degree Disjoyne us, and I then too late renounce Deitie for thee, when Fate will not permit. Thus Eve with Countnance blithe her storie told; But in her Cheek distemper flushing glowd. On th' other side, Adam, soon as he heard The fatal Trespass don by Eve, amaz'd, Astonied stood and Blank, while horror chill Ran through his veins, and all his joynts relax'd; From his slack hand the Garland wreath'd for Eve Down drop'd, and all the faded Roses shed: Speechless he stood and pale, till thus at length First to himself he inward silence broke. O fairest of Creation, last and best Of all Gods works, Creature in whom excell'd Whatever can to sight or thought be formd, Holy, divine, good, amiable, or sweet! How art thou lost, how on a sudden lost, Defac't, deflourd, and now to Death devote? Rather how hast thou yeelded to transgress The strict forbiddance, how to violate The sacred Fruit forbidd'n! som cursed fraud Of Enemie hath beguil'd thee, yet unknown, And mee with thee hath ruind, for with thee Certain my resolution is to Die; How can I live without thee, how forgoe Thy sweet Converse and Love so dearly joyn'd, To live again in these wilde Woods forlorn? Should God create another Eve, and I Another Rib afford, yet loss of thee Would never from my heart; no no, I feel The Link of Nature draw me: Flesh of Flesh, Bone of my Bone thou art, and from thy State Mine never shall be parted, bliss or woe. So having said, as one from sad dismay Recomforted, and after thoughts disturbd Submitting to what seemd remediless, Thus in calm mood his Words to Eve he turnd. Bold deed thou hast presum'd, adventrous Eve, And peril great provok't, who thus hath dar'd Had it been onely coveting to Eye That sacred Fruit, sacred to abstinence, Much more to taste it under banne to touch. But past who can recall, or don undoe? Not God Omnipotent, nor Fate, yet so Perhaps thou shalt not Die, perhaps the Fact Is not so hainous now, foretasted Fruit, Profan'd first by the Serpent, by him first Made common and unhallowd ere our taste; Nor yet on him found deadly, he yet lives, Lives, as thou saidst, and gaines to live as Man Higher degree of Life, inducement strong To us, as likely tasting to attaine Proportional ascent, which cannot be But to be Gods, or Angels Demi-gods. Nor can I think that God, Creator wise, Though threatning, will in earnest so destroy Us his prime Creatures, dignifi'd so high, Set over all his Works, which in our Fall, For us created, needs with us must faile, Dependent made; so God shall uncreate, Be frustrate, do, undo, and labour loose, Not well conceav'd of God, who though his Power Creation could repeate, yet would be loath Us to abolish, least the Adversary Triumph and say; Fickle their State whom God Most Favors, who can please him long; Mee first He ruind, now Mankind; whom will he next? Matter of scorne, not to be given the Foe, However I with thee have fixt my Lot, Certain to undergoe like doom, if Death Consort with thee, Death is to mee as Life; So forcible within my heart I feel The Bond of Nature draw me to my owne, My own in thee, for what thou art is mine; Our State cannot be severd, we are one, One Flesh; to loose thee were to loose my self. So Adam, and thus Eve to him repli'd. O glorious trial of exceeding Love, Illustrious evidence, example high! Ingaging me to emulate, but short Of thy perfection, how shall I attaine, Adam, from whose deare side I boast me sprung, And gladly of our Union heare thee speak, One Heart, one Soul in both; whereof good prooff This day affords, declaring thee resolvd, Rather then Death or aught then Death more dread Shall separate us, linkt in Love so deare, To undergoe with mee one Guilt, one Crime, If any be, of tasting this fair Fruit, Whose vertue, for of good still good proceeds, Direct, or by occasion hath presented This happie trial of thy Love, which else So eminently never had bin known. Were it I thought Death menac't would ensue This my attempt, I would sustain alone The worst, and not perswade thee, rather die Deserted, then oblige thee with a fact Pernicious to thy Peace, chiefly assur'd Remarkably so late of thy so true, So faithful Love unequald; but I feel Farr otherwise th' event, not Death, but Life Augmented, op'nd Eyes, new Hopes, new Joyes, Taste so Divine, that what of sweet before Hath toucht my sense, flat seems to this, and harsh. On my experience, Adam, freely taste, And fear of Death deliver to the Windes. So saying, she embrac'd him, and for joy Tenderly wept, much won that he his Love Had so enobl'd, as of choice to incurr Divine displeasure for her sake, or Death. In recompence (for such compliance bad Such recompence best merits) from the bough She gave him of that fair enticing Fruit With liberal hand: he scrupl'd not to eat Against his better knowledge, not deceav'd, But fondly overcome with Femal charm. Earth trembl'd from her entrails, as again In pangs, and Nature gave a second groan, Skie lowr'd and muttering Thunder, som sad drops Wept at compleating of the mortal Sin Original; while Adam took no thought, Eating his fill, nor Eve to iterate Her former trespass fear'd, the more to soothe Him with her lov'd societie, that now As with new Wine intoxicated both They swim in mirth, and fansie that they feel Divinitie within them breeding wings Wherewith to scorne the Earth: but that false Fruit Farr other operation first displaid, Carnal desire enflaming, hee on Eve Began to cast lascivious Eyes, she him As wantonly repaid; in Lust they burne: Till Adam thus'gan Eve to dalliance move,Eve, now I see thou art exact of taste, And elegant, of Sapience no small part, Since to each meaning savour we apply, And Palate call judicious; I the praise Yeild thee, so well this day thou hast purvey'd. Much pleasure we have lost, while we abstain'd From this delightful Fruit, nor known till now True relish, tasting; if such pleasure be In things to us forbidden, it might be wish'd, For this one Tree had bin forbidden ten. But come, so well refresh't, now let us play, As meet is, after such delicious Fare; For never did thy Beautie since the day I saw thee first and wedded thee, adorn'd With all perfections, so enflame my sense With ardor to enjoy thee, fairer now Then ever, bountie of this vertuous Tree. So said he, and forbore not glance or toy Of amorous intent, well understood Of Eve, whose Eye darted contagious Fire. Her hand he seis'd, and to a shadie bank, Thick overhead with verdant roof imbowr'd He led her nothing loath; Flours were the Couch, Pansies, and Violets, and Asphodel, And Hyacinth, Earths freshest softest lap. There they thir fill of Love and Loves disport Took largely, of thir mutual guilt the Seale, The solace of thir sin, till dewie sleep Oppress'd them, wearied with thir amorous play. Soon as the force of that fallacious Fruit, That with exhilerating vapour bland About thir spirits had plaid, and inmost powers Made erre, was now exhal'd, and grosser sleep Bred of unkindly fumes, with conscious dreams Encumberd, now had left them, up they rose As from unrest, and each the other viewing, Soon found thir Eyes how op'nd, and thir minds How dark'nd; innocence, that as a veile Had shadow'd them from knowing ill, was gon, Just confidence, and native righteousness And honour from about them, naked left To guiltie shame hee cover'd, but his Robe Uncover'd more, so rose the Danite strongHerculean Samson from the Harlot-lap Of Philistean Dalilah, and wak'd Shorn of his strength, They destitute and bare Of all thir vertue: silent, and in face Confounded long they sate, as struck'n mute, Till Adam, though not less then Eve abash't, At length gave utterance to these words constraind.Eve, in evil hour thou didst give eare To that false Worm, of whomsoever taught To counterfet Mans voice, true in our Fall, False in our promis'd Rising; since our Eyes Op'nd we find indeed, and find we know Both Good and Evil, Good lost, and Evil got, Bad Fruit of Knowledge, if this be to know, Which leaves us naked thus, of Honour void, Of Innocence, of Faith, of Puritie, Our wonted Ornaments now soild and staind, And in our Faces evident the signes Of foul concupiscence; whence evil store; Even shame, the last of evils; of the first Be sure then. How shall I behold the face Henceforth of God or Angel, earst with joy And rapture so oft beheld? those heav'nly shapes Will dazle now this earthly, with thir blaze Insufferably bright. O might I here In solitude live savage, in some glade Obscur'd, where highest Woods impenetrable To Starr or Sun-light, spread thir umbrage broad And brown as Evening: Cover me ye Pines, Ye Cedars, with innumerable boughs Hide me, where I may never see them more. But let us now, as in bad plight, devise What best may from the present serve to hide The Parts of each for other, that seem most To shame obnoxious, and unseemliest seen, Some Tree whose broad smooth Leaves together sowd, And girded on our loyns, may cover round Those middle parts, that this new commer, Shame, There sit not, and reproach us as unclean. So counsel'd hee, and both together went Into the thickest Wood, there soon they chose The Figtree, not that kind for Fruit renown'd, But such as at this day to Indians known In Malabar or Decan spreds her Armes Braunching so broad and long, that in the ground The bended Twigs take root, and Daughters grow About the Mother Tree, a Pillard shade High overarch't, and echoing Walks between; There oft the Indian Herdsman shunning heate Shelters in coole, and tends his pasturing Herds At Loopholes cut through thickest shade: Those Leaves They gatherd, broad as Amazonian Targe, And with what skill they had, together sowd, To gird thir waste, vain Covering if to hide Thir guilt and dreaded shame; O how unlike To that first naked Glorie. Such of lateColumbus found th' American so girt With featherd Cincture, naked else and wilde Among the Trees on Iles and woodie Shores. Thus fenc't, and as they thought, thir shame in part Coverd, but not at rest or ease of Mind, They sate them down to weep, nor onely Teares Raind at thir Eyes, but high Winds worse within Began to rise, high Passions, Anger, Hate, Mistrust, Suspicion, Discord, and shook sore Thir inward State of Mind, calm Region once And full of Peace, now tost and turbulent: For Understanding rul'd not, and the Will Heard not her lore, both in subjection now To sensual Appetite, who from beneathe Usurping over sovran Reason claimd Superior sway: from thus distemperd brest,Adam, estrang'd in look and alterd stile, Speech intermitted thus to Eve renewd. Would thou hadst heark'nd to my words, and stai'd With me, as I besought thee, when that strange Desire of wandring this unhappie Morn, I know not whence possessd thee; we had then Remaind still happie, not as now, despoild Of all our good, sham'd, naked, miserable. Let none henceforth seek needless cause to approve The Faith they owe; when earnestly they seek Such proof, conclude, they then begin to faile. To whom soon mov'd with touch of blame thus Eve. What words have past thy Lips, Adam severe, Imput'st thou that to my default, or will Of wandring, as thou call'st it, which who knows But might as ill have happ'nd thou being by, Or to thy self perhaps: hadst thou been there, Or here th' attempt, thou couldst not have discernd Fraud in the Serpent, speaking as he spake; No ground of enmitie between us known, Why hee should mean me ill, or seek to harme, Was I to have never parted from thy side? As good have grown there still a liveless Rib. Being as I am, why didst not thou the Head Command me absolutely not to go, Going into such danger as thou saidst? Too facil then thou didst not much gainsay, Nay didst permit, approve, and fair dismiss. Hadst thou bin firm and fixt in thy dissent, Neither had I transgress'd, nor thou with mee. To whom then first incenst Adam repli'd, Is this the Love, is this the recompence Of mine to thee, ingrateful Eve, exprest Immutable when thou wert lost, not I, Who might have liv'd and joyd immortal bliss, Yet willingly chose rather Death with thee: And am I now upbraided, as the cause Of thy transgressing? not enough severe, It seems, in thy restraint: what could I more? I warn'd thee, I admonish'd thee, foretold The danger, and the lurking Enemie That lay in wait; beyond this had bin force, And force upon free will hath here no place. But confidence then bore thee on, secure Either to meet no danger, or to finde Matter of glorious trial; and perhaps I also err'd in overmuch admiring What seemd in thee so perfet, that I thought No evil durst attempt thee, but I rue That errour now, which is become my crime, And thou th' accuser. Thus it shall befall Him who to worth in Women overtrusting Lets her will rule; restraint she will not brook, And left to her self, if evil thence ensue, Shee first his weak indulgence will accuse. Thus they in mutual accusation spent The fruitless hours, but neither self-condemning, And of thir vain contest appeer'd no end.\"]\n"
     ]
    }
   ],
   "source": [
    "print(combined.sample(5)[\"text\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36601c1c-770e-49ef-ad26-ab226dedb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = combined[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdd03a6a-cadd-4c1a-af2c-43df623b6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4538940-2f3f-4a57-a039-eb1986138992",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = [tokenizer.encode(doc) for doc in docs]\n",
    "tokenized_docs = [tokens for tokens in tokenized_docs if len(tokens)>32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9b6d202-5f7a-45e4-ae6b-0d1325838bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = GPT_CONFIG_124M[\"context_length\"]\n",
    "\n",
    "def chunk_tokens(tokens, block_size=context_len):\n",
    "    return [tokens[i:i + block_size] for i in range(0, len(tokens) - block_size)]\n",
    "\n",
    "all_chunks = []\n",
    "for tokens in tokenized_docs:\n",
    "    all_chunks.extend(chunk_tokens(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f27653a-828c-4009-8868-79901decd0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTPoetryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, chunks):\n",
    "        self.data = chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx][:-1], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx][1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "dataset = GPTPoetryDataset(all_chunks)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f936c74-2747-4bef-bb6a-8d1d154dbafa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_1(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): Layernorm()\n",
       "      (norm2): Layernorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): Layernorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = torch.optim.Adam(gpt.parameters(), lr=3e-5)  # Learning rate can be tuned\n",
    "gpt.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "702ca3f7-ebf5-4eeb-b2ee-32f819bf94e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.98 GiB is allocated by PyTorch, and 53.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m      6\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m, in \u001b[0;36mModel_1.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_embeds \u001b[38;5;241m+\u001b[39m pos_embeds\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_emb(x)\n\u001b[1;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrf_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(x)\n\u001b[0;32m     18\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_head(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m shortcut\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m attn_score \u001b[38;5;241m=\u001b[39m queries \u001b[38;5;241m@\u001b[39m keys\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     26\u001b[0m masked \u001b[38;5;241m=\u001b[39m attn_score\u001b[38;5;241m.\u001b[39mmasked_fill_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mbool()[:num_tokens, :num_tokens], \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m---> 27\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mmasked\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_weights)\n\u001b[0;32m     29\u001b[0m context_vec \u001b[38;5;241m=\u001b[39m (attn_weights \u001b[38;5;241m@\u001b[39m values)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.98 GiB is allocated by PyTorch, and 53.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "num_epochs = 1 # You can increase this for better results\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for step, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits = gpt(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} finished, Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2a43a-f46c-4ada-9deb-e428b08540b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt.state_dict(), \"fine_tuned_ai_poetry.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a589f20-743d-43d4-a3b6-6d3606e65748",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config = GPT_CONFIG_124M.copy()\n",
    "new_config.update({\"qkv_bias\": True})  # Must match training config\n",
    "\n",
    "gpt = Model_1(new_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388c99a-364a-4c5c-ab34-1d2212ef0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.load_state_dict(torch.load(\"fine_tuned_ai_poetry.pth\", map_location=\"cpu\"))  # or map_location=\"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19c81f-3f8b-4036-8b3e-249cb1e7bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device)\n",
    "gpt.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d3106-f812-4286-af22-17cb0d64e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "prompt = \"Write a romantic poem about the moonlight\"\n",
    "input_ids = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long).to(device)\n",
    "output_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    context_size=new_config[\"context_length\"],\n",
    "    temperature=0.8,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "generated_poem_ids = output_ids[:, input_ids.shape[1]:]\n",
    "generated_poem = tokenizer.decode(generated_poem_ids[0].tolist())\n",
    "\n",
    "print(generated_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3692194-36ff-4a4b-a0c9-246542332b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
